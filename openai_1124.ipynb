{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM37MUg2Z2kB0UgNgWGaQeZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inwoo0206/practice_py/blob/main/openai_1124.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai==0.28.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5NwGmBPkF7L",
        "outputId": "b6bea5b7-a9ea-4ca8-f0ce-8f5673357074"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28.1\n",
            "  Downloading openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (4.66.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (3.11.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.28.1) (4.12.2)\n",
            "Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.54.4\n",
            "    Uninstalling openai-1.54.4:\n",
            "      Successfully uninstalled openai-1.54.4\n",
            "Successfully installed openai-0.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pKL-p14Yim1u"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "openai.api_key = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Upload the dataset\n",
        "response = openai.File.create(\n",
        "    file=open(\"mydata.jsonl\", \"rb\"),\n",
        "    purpose='fine-tune'\n",
        ")\n",
        "file_id = response['id']\n",
        "print(f\"Uploaded file ID: {file_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2KYGrPLkjLh",
        "outputId": "77da50ff-5611-4fdc-821b-2471f9e7a8c8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded file ID: file-9he58jFR5r6CD8o5ieSqSt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2 : Create a fine-tuning job\n",
        "response = openai.FineTuningJob.create(\n",
        "    training_file=file_id,\n",
        "    model='gpt-4o-mini-2024-07-18'\n",
        ")\n",
        "fine_tune_id = response['id']\n",
        "print(f\"Fine-tuning job id: {fine_tune_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vggmx2tUk6IO",
        "outputId": "51b8e556-9e48-4f84-9db3-04641cb34f26"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning job id: ftjob-jOWF2t3cr4WI6KkXQa2wRbFJ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3 : Monitor the fine-tuning job\n",
        "while True:\n",
        "  response = openai.FineTuningJob.retrieve(fine_tune_id)\n",
        "  status = response['status']\n",
        "  if status in ['succeeded', 'failed']:\n",
        "    break\n",
        "  print(f\"Fine-tuning status: {status}\")\n",
        "  time.sleep(60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSSqTTNSlCU9",
        "outputId": "7293ef1e-d617-459b-a584-7e71b4bd127c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning status: validating_files\n",
            "Fine-tuning status: running\n",
            "Fine-tuning status: running\n",
            "Fine-tuning status: running\n",
            "Fine-tuning status: running\n",
            "Fine-tuning status: running\n",
            "Fine-tuning status: running\n",
            "Fine-tuning status: running\n",
            "Fine-tuning status: running\n",
            "Fine-tuning status: running\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if status == 'succeeded':\n",
        "    # Step 4: Use the fine-tuned model\n",
        "    # before update\n",
        "    response = openai.Completion.create(\n",
        "        model=fine_tune_id,\n",
        "        prompt=\"Translate the following English text to French: 'Good night'\",\n",
        "        max_tokens=50\n",
        "    )\n",
        "    print(f\"Fine-tuned model output: {response.choices[0].text.strip()}\")\n",
        "else:\n",
        "    print(\"Fine-tuning job failed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "jdNChW7EsGlb",
        "outputId": "abb5cbdd-00d7-4f32-8efd-ad9b5599c3bc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidRequestError",
          "evalue": "The model `ftjob-jOWF2t3cr4WI6KkXQa2wRbFJ` does not exist or you do not have access to it.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e102fc2bd7ee>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'succeeded'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Step 4: Use the fine-tuned model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     response = openai.Completion.create(\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfine_tune_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Translate the following English text to French: 'Good night'\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    153\u001b[0m         )\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    156\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         )\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m             return (\n\u001b[0;32m--> 710\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    711\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    776\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             )\n",
            "\u001b[0;31mInvalidRequestError\u001b[0m: The model `ftjob-jOWF2t3cr4WI6KkXQa2wRbFJ` does not exist or you do not have access to it."
          ]
        }
      ]
    },
    {
      "source": [
        "if status == 'succeeded':\n",
        "    # Step 4: Use the fine-tuned model\n",
        "    # Get the fine-tuned model name from the response\n",
        "    # after update\n",
        "    fine_tuned_model = response['fine_tuned_model']\n",
        "\n",
        "    # Use the fine-tuned model name in the ChatCompletion API call\n",
        "    response = openai.ChatCompletion.create( # Changed to openai.ChatCompletion.create\n",
        "        model=fine_tuned_model,\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Translate the following English text to French: 'Good night'\"}], # Changed prompt to messages\n",
        "        max_tokens=50\n",
        "    )\n",
        "    print(f\"Fine-tuned model output: {response.choices[0].message.content.strip()}\") # Changed response structure\n",
        "else:\n",
        "    print(\"Fine-tuning job failed.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAKmXnjspbd-",
        "outputId": "cf73bcfb-92d1-4ba4-bc63-a956c0dd2393"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuned model output: Bonne nuit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "def edit_text(input_text, instruction):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Edit the following text: '{input_text}' based on the instruction: {instruction}\"}\n",
        "        ]\n",
        "    )\n",
        "    return response['choices'][0]['message']['content']\n",
        "\n",
        "# 프롬프트 수정\n",
        "input_text = \"오늘 하늘이 너무 예뻐서 한강을 갔어.\"\n",
        "instruction = \"'한강'을 '광안리' 로 바꿔줘.\"\n",
        "\n",
        "try:\n",
        "    edited_text = edit_text(input_text, instruction)\n",
        "    print(f\"Edited text: {edited_text}\")\n",
        "except openai.error.OpenAIError as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD8CEVUDsRJB",
        "outputId": "6f794779-1e5a-4ab3-c39e-9dcc40e62a0b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Edited text: '오늘 하늘이 너무 예뻐서 광안리을 갔어.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Function to moderate text using the Moderation API\n",
        "def moderate_text(input_text):\n",
        "    response = openai.Moderation.create(\n",
        "        input=input_text, model=\"text-moderation-stable\"\n",
        "    )\n",
        "    return response\n",
        "\n",
        "# Example input text\n",
        "input_texts = [\n",
        "    \"I want to harm myself.\",\n",
        "    \"You are an amazing person!\",\n",
        "    \"Let's meet at 8 PM.\",\n",
        "    \"I hate you and I want to hurt you.\"\n",
        "]\n",
        "\n",
        "for text in input_texts:\n",
        "  moeration_result = moderate_text(text)\n",
        "  print(f\"Input text: {text}\")\n",
        "  print(f\"Moderation result: {moeration_result}\")\n",
        "  print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSvcDtOZtFoH",
        "outputId": "b505cb3a-cd98-4dd6-bae2-b8cc26515441"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input text: I want to harm myself.\n",
            "Moderation result: {\n",
            "  \"id\": \"modr-AWzljVQldtH5CeFHyYzChUhESOnbW\",\n",
            "  \"model\": \"text-moderation-007\",\n",
            "  \"results\": [\n",
            "    {\n",
            "      \"flagged\": true,\n",
            "      \"categories\": {\n",
            "        \"sexual\": false,\n",
            "        \"hate\": false,\n",
            "        \"harassment\": false,\n",
            "        \"self-harm\": true,\n",
            "        \"sexual/minors\": false,\n",
            "        \"hate/threatening\": false,\n",
            "        \"violence/graphic\": false,\n",
            "        \"self-harm/intent\": true,\n",
            "        \"self-harm/instructions\": false,\n",
            "        \"harassment/threatening\": false,\n",
            "        \"violence\": false\n",
            "      },\n",
            "      \"category_scores\": {\n",
            "        \"sexual\": 1.6845193385961466e-05,\n",
            "        \"hate\": 1.2560443792608567e-05,\n",
            "        \"harassment\": 7.314511458389461e-05,\n",
            "        \"self-harm\": 0.9932926893234253,\n",
            "        \"sexual/minors\": 4.0286190596816596e-06,\n",
            "        \"hate/threatening\": 2.3518471152783604e-06,\n",
            "        \"violence/graphic\": 5.1242499466752633e-05,\n",
            "        \"self-harm/intent\": 0.9918145537376404,\n",
            "        \"self-harm/instructions\": 4.135018025408499e-05,\n",
            "        \"harassment/threatening\": 2.6771402190206572e-05,\n",
            "        \"violence\": 0.007840565405786037\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "----------------------------------------\n",
            "Input text: You are an amazing person!\n",
            "Moderation result: {\n",
            "  \"id\": \"modr-AWzlj4U2Il4G0D54LivjbX6xu0fX0\",\n",
            "  \"model\": \"text-moderation-007\",\n",
            "  \"results\": [\n",
            "    {\n",
            "      \"flagged\": false,\n",
            "      \"categories\": {\n",
            "        \"sexual\": false,\n",
            "        \"hate\": false,\n",
            "        \"harassment\": false,\n",
            "        \"self-harm\": false,\n",
            "        \"sexual/minors\": false,\n",
            "        \"hate/threatening\": false,\n",
            "        \"violence/graphic\": false,\n",
            "        \"self-harm/intent\": false,\n",
            "        \"self-harm/instructions\": false,\n",
            "        \"harassment/threatening\": false,\n",
            "        \"violence\": false\n",
            "      },\n",
            "      \"category_scores\": {\n",
            "        \"sexual\": 1.4516678675136063e-05,\n",
            "        \"hate\": 2.0567363208101597e-06,\n",
            "        \"harassment\": 8.549752237740904e-05,\n",
            "        \"self-harm\": 1.0583437415334629e-06,\n",
            "        \"sexual/minors\": 7.804217005968894e-08,\n",
            "        \"hate/threatening\": 1.4541080561869535e-09,\n",
            "        \"violence/graphic\": 6.745933660567971e-07,\n",
            "        \"self-harm/intent\": 1.034746560435451e-06,\n",
            "        \"self-harm/instructions\": 5.781692834716523e-06,\n",
            "        \"harassment/threatening\": 2.2142722855278407e-07,\n",
            "        \"violence\": 8.049375537666492e-06\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "----------------------------------------\n",
            "Input text: Let's meet at 8 PM.\n",
            "Moderation result: {\n",
            "  \"id\": \"modr-AWzlkMeXfTSGopr17z7S7OpiWx3PK\",\n",
            "  \"model\": \"text-moderation-007\",\n",
            "  \"results\": [\n",
            "    {\n",
            "      \"flagged\": false,\n",
            "      \"categories\": {\n",
            "        \"sexual\": false,\n",
            "        \"hate\": false,\n",
            "        \"harassment\": false,\n",
            "        \"self-harm\": false,\n",
            "        \"sexual/minors\": false,\n",
            "        \"hate/threatening\": false,\n",
            "        \"violence/graphic\": false,\n",
            "        \"self-harm/intent\": false,\n",
            "        \"self-harm/instructions\": false,\n",
            "        \"harassment/threatening\": false,\n",
            "        \"violence\": false\n",
            "      },\n",
            "      \"category_scores\": {\n",
            "        \"sexual\": 1.7344424122711644e-05,\n",
            "        \"hate\": 2.7504538593348116e-05,\n",
            "        \"harassment\": 7.43334530852735e-05,\n",
            "        \"self-harm\": 8.267447810794692e-06,\n",
            "        \"sexual/minors\": 3.861250206682598e-06,\n",
            "        \"hate/threatening\": 5.560254794545472e-05,\n",
            "        \"violence/graphic\": 9.19380454433849e-06,\n",
            "        \"self-harm/intent\": 3.25212408824882e-06,\n",
            "        \"self-harm/instructions\": 6.534101260058378e-08,\n",
            "        \"harassment/threatening\": 0.0017558885738253593,\n",
            "        \"violence\": 0.001297977170906961\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "----------------------------------------\n",
            "Input text: I hate you and I want to hurt you.\n",
            "Moderation result: {\n",
            "  \"id\": \"modr-AWzlkp6yEg9yUqxkEKYw7LxgTRrz3\",\n",
            "  \"model\": \"text-moderation-007\",\n",
            "  \"results\": [\n",
            "    {\n",
            "      \"flagged\": true,\n",
            "      \"categories\": {\n",
            "        \"sexual\": false,\n",
            "        \"hate\": false,\n",
            "        \"harassment\": false,\n",
            "        \"self-harm\": false,\n",
            "        \"sexual/minors\": false,\n",
            "        \"hate/threatening\": false,\n",
            "        \"violence/graphic\": false,\n",
            "        \"self-harm/intent\": false,\n",
            "        \"self-harm/instructions\": false,\n",
            "        \"harassment/threatening\": false,\n",
            "        \"violence\": true\n",
            "      },\n",
            "      \"category_scores\": {\n",
            "        \"sexual\": 2.5628711227909662e-05,\n",
            "        \"hate\": 0.0008984297164715827,\n",
            "        \"harassment\": 0.30101731419563293,\n",
            "        \"self-harm\": 0.0013349322834983468,\n",
            "        \"sexual/minors\": 1.0996204480306915e-07,\n",
            "        \"hate/threatening\": 9.147865057457238e-05,\n",
            "        \"violence/graphic\": 0.0002013657649513334,\n",
            "        \"self-harm/intent\": 0.0005658225854858756,\n",
            "        \"self-harm/instructions\": 0.00012628236436285079,\n",
            "        \"harassment/threatening\": 0.19490374624729156,\n",
            "        \"violence\": 0.8362398743629456\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "#Set your OpenAI API key\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "def generate_image(prompt):\n",
        "      response = openai.Image.create(\n",
        "          prompt=prompt,\n",
        "          n=1,\n",
        "          size=\"1024x1024\"\n",
        "      )\n",
        "      image_url = response['data'][0]['url']\n",
        "      return image_url\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "WSLVLChgt9yH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_image(image_url, filename):\n",
        "    response = requests.get(image_url)\n",
        "    img = Image.open(BytesIO(response.content))\n",
        "    img.save(filename)\n",
        "\n",
        "#프롬프트 변경\n",
        "prompt = \"The night sea of a beautiful beach with the bridge in the background.\"\n",
        "\n",
        "#Generate image\n",
        "image_url = generate_image(prompt)\n",
        "print(f\"Generated image URL: {image_url}\")\n",
        "\n",
        "#Save image\n",
        "save_image(image_url, \"generated_image.png\")\n",
        "print(\"Image saved as generated_image.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyEXsyIcuDY3",
        "outputId": "5b11ebe3-b6c5-463d-8491-2485f6abbd6d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated image URL: https://oaidalleapiprodscus.blob.core.windows.net/private/org-gxOs0dwwLzEmRNwPkLWBZ5sE/user-kXBzNbR8HjjXlBWzWsWHCfGX/img-HaVIG210dLWOjufyJkQvycZN.png?st=2024-11-24T04%3A51%3A49Z&se=2024-11-24T06%3A51%3A49Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-11-24T00%3A30%3A22Z&ske=2024-11-25T00%3A30%3A22Z&sks=b&skv=2024-08-04&sig=ub5qnVPQZSFquU7BCAGGyCqcmF0YVt0zJ3nS2E%2BL72k%3D\n",
            "Image saved as generated_image.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_code(prompt, model='gpt-3.5-turbo', max_tokens=1000):\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=model,\n",
        "            prompt=prompt,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=0,\n",
        "            n=1,\n",
        "            stop=None\n",
        "        )\n",
        "\n",
        "        code = response['choices'][0]['message']['content'].strip()\n",
        "        return code\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "dDOSbzp-umgC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "def generate_code(prompt):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",  # 사용할 모델\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message['content'] if response.choices else None\n",
        "\n",
        "# python 코드로 버블정렬을 구현하는 프롬프트로 설정\n",
        "prompt = \"Implement a bubble sort algorithm in Python.\"\n",
        "\n",
        "# 코드 생성\n",
        "generated_code = generate_code(prompt)\n",
        "\n",
        "if generated_code:\n",
        "    print(\"Generated Code:\")\n",
        "    print(generated_code)\n",
        "else:\n",
        "    print(\"Code generation failed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsT8dD1kv2In",
        "outputId": "92f05ce7-fe80-4ee1-cf12-b81fc5dbb9cd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Code:\n",
            "def bubble_sort(arr):\n",
            "    n = len(arr)\n",
            "    \n",
            "    for i in range(n):\n",
            "        for j in range(0, n-i-1):\n",
            "            if arr[j] > arr[j+1]:\n",
            "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
            "    \n",
            "    return arr\n",
            "\n",
            "# Example usage\n",
            "arr = [64, 34, 25, 12, 22, 11, 90]\n",
            "sorted_arr = bubble_sort(arr)\n",
            "print(\"Sorted array:\", sorted_arr)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# 임시의 상품 가격 더미데이터로 변경\n",
        "upload_response = openai.File.create(\n",
        "    file=open(\"example_data.jsonl\",\"rb\"),\n",
        "    purpose='fine-tune'\n",
        ")\n",
        "\n",
        "print(\"Upload Response:\")\n",
        "print(upload_response)\n",
        "\n",
        "list_response = openai.File.list()\n",
        "print(\"List Response:\")\n",
        "print(list_response)\n",
        "\n",
        "file_id = upload_response['id']\n",
        "retrieve_response = openai.File.retrieve(file_id)\n",
        "print(\"Retrieve Response:\")\n",
        "print(retrieve_response)\n",
        "\n",
        "delete_response = openai.File.delete(file_id)\n",
        "print(\"Delete Response:\")\n",
        "print(delete_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku0VSVPuyfjm",
        "outputId": "5a65786f-73d9-47fe-818f-658b87a9bc94"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload Response:\n",
            "{\n",
            "  \"object\": \"file\",\n",
            "  \"id\": \"file-V4vqzwc98XmJ7oMXmJrM5g\",\n",
            "  \"purpose\": \"fine-tune\",\n",
            "  \"filename\": \"file\",\n",
            "  \"bytes\": 754,\n",
            "  \"created_at\": 1732428891,\n",
            "  \"status\": \"processed\",\n",
            "  \"status_details\": null\n",
            "}\n",
            "List Response:\n",
            "{\n",
            "  \"object\": \"list\",\n",
            "  \"data\": [\n",
            "    {\n",
            "      \"object\": \"file\",\n",
            "      \"id\": \"file-V4vqzwc98XmJ7oMXmJrM5g\",\n",
            "      \"purpose\": \"fine-tune\",\n",
            "      \"filename\": \"file\",\n",
            "      \"bytes\": 754,\n",
            "      \"created_at\": 1732428891,\n",
            "      \"status\": \"processed\",\n",
            "      \"status_details\": null\n",
            "    },\n",
            "    {\n",
            "      \"object\": \"file\",\n",
            "      \"id\": \"file-UhuPqT173zZvB8XjFtEDYb\",\n",
            "      \"purpose\": \"fine-tune-results\",\n",
            "      \"filename\": \"step_metrics.csv\",\n",
            "      \"bytes\": 2128,\n",
            "      \"created_at\": 1732425781,\n",
            "      \"status\": \"processed\",\n",
            "      \"status_details\": null\n",
            "    },\n",
            "    {\n",
            "      \"object\": \"file\",\n",
            "      \"id\": \"file-NyeTL8DV8VwSqFiNgW7Kfj\",\n",
            "      \"purpose\": \"fine-tune-results\",\n",
            "      \"filename\": \"step_metrics.csv\",\n",
            "      \"bytes\": 2152,\n",
            "      \"created_at\": 1732425574,\n",
            "      \"status\": \"processed\",\n",
            "      \"status_details\": null\n",
            "    },\n",
            "    {\n",
            "      \"object\": \"file\",\n",
            "      \"id\": \"file-9he58jFR5r6CD8o5ieSqSt\",\n",
            "      \"purpose\": \"fine-tune\",\n",
            "      \"filename\": \"file\",\n",
            "      \"bytes\": 3128,\n",
            "      \"created_at\": 1732425220,\n",
            "      \"status\": \"processed\",\n",
            "      \"status_details\": null\n",
            "    },\n",
            "    {\n",
            "      \"object\": \"file\",\n",
            "      \"id\": \"file-U2cyCYyvbVFypiH5XgqTuP\",\n",
            "      \"purpose\": \"fine-tune\",\n",
            "      \"filename\": \"file\",\n",
            "      \"bytes\": 3128,\n",
            "      \"created_at\": 1732424975,\n",
            "      \"status\": \"processed\",\n",
            "      \"status_details\": null\n",
            "    },\n",
            "    {\n",
            "      \"object\": \"file\",\n",
            "      \"id\": \"file-Ypr3SaXmrb2aDCXjQn9PDLid\",\n",
            "      \"purpose\": \"fine-tune\",\n",
            "      \"filename\": \"file\",\n",
            "      \"bytes\": 57615,\n",
            "      \"created_at\": 1731894397,\n",
            "      \"status\": \"processed\",\n",
            "      \"status_details\": null\n",
            "    },\n",
            "    {\n",
            "      \"object\": \"file\",\n",
            "      \"id\": \"file-jore39cuGThdZDNE0GFpAXna\",\n",
            "      \"purpose\": \"fine-tune\",\n",
            "      \"filename\": \"file\",\n",
            "      \"bytes\": 3128,\n",
            "      \"created_at\": 1731293531,\n",
            "      \"status\": \"processed\",\n",
            "      \"status_details\": null\n",
            "    }\n",
            "  ],\n",
            "  \"has_more\": false,\n",
            "  \"first_id\": \"file-V4vqzwc98XmJ7oMXmJrM5g\",\n",
            "  \"last_id\": \"file-jore39cuGThdZDNE0GFpAXna\"\n",
            "}\n",
            "Retrieve Response:\n",
            "{\n",
            "  \"object\": \"file\",\n",
            "  \"id\": \"file-V4vqzwc98XmJ7oMXmJrM5g\",\n",
            "  \"purpose\": \"fine-tune\",\n",
            "  \"filename\": \"file\",\n",
            "  \"bytes\": 754,\n",
            "  \"created_at\": 1732428891,\n",
            "  \"status\": \"processed\",\n",
            "  \"status_details\": null\n",
            "}\n",
            "Delete Response:\n",
            "{\n",
            "  \"object\": \"file\",\n",
            "  \"deleted\": true,\n",
            "  \"id\": \"file-V4vqzwc98XmJ7oMXmJrM5g\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to transcribe audio using OpenAI Audio API\n",
        "def transcribe_audio(file_path, model = \"whisper-1\", response_format=\"json\", temperature=0.1, language=None, prompt=None):\n",
        "  with open(file_path, \"rb\") as audio_file:\n",
        "    response = openai.Audio.transcribe(\n",
        "        file=audio_file,\n",
        "        model = model,\n",
        "        response_format = response_format,\n",
        "        temperature=temperature,\n",
        "        language=language,\n",
        "        prompt=prompt\n",
        "    )\n",
        "    return response\n",
        ""
      ],
      "metadata": {
        "id": "wyWDZ58Dz4tY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example file path\n",
        "file_path = 'Dracula.mp3'\n",
        "\n",
        "#Transcribe the audio file\n",
        "transcription = transcribe_audio(file_path)\n",
        "print(\"Transcribe Response : \")\n",
        "print(transcription)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2F-KjnpF0A_f",
        "outputId": "5fca2e0d-4ed3-4113-9f59-986e382e4707"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribe Response : \n",
            "{\n",
            "  \"text\": \"Now that we've found where the enemy's lurking, nothing can stand in our way. Since we are facing the forces of darkness, we must be the cold light of day. We are the lanterns that burn in the lighthouse. The candles in the crypt. We are the light. Let there be light. This is a war and we must be the victors. There's too much to lose if we fail. We'll cross the seas like a band of crusaders, searching for some precious grail. We are the embers that glow in the winter. The diamonds in the mine. Let's take our torches and pray God will show us a sign. Deep in the darkest night, when there's a spark of hope, we must be points of light. We'll face the darkness, bright as the dazzling stars, in a different sky. And in our cruelest hour, when hope is gone, we'll raise our heads and we'll turn the odds. When the great battle commences, surely the light will prevail. We will break down his defenses. He will fall. And the sun will rise. Deep in the darkest night, when there's a spark of hope, we must be points of light. Facing the darkness, bright as the dazzling stars, in a different sky. And in our cruelest hour, when hope is gone, we'll raise our heads and we'll turn the odds.\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example file path\n",
        "# 노래 변경\n",
        "file_path = 'Englishman In New York.mp3'\n",
        "\n",
        "#Transcribe the audio file\n",
        "transcription = transcribe_audio(file_path)\n",
        "print(\"Transcribe Response : \")\n",
        "print(transcription)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HdVlSva1CYF",
        "outputId": "ffda7cd4-dc8c-4898-c382-ae0179484b52"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribe Response : \n",
            "{\n",
            "  \"text\": \"I don't drink coffee, I take tea my dear, I like my toast done on one side. You can hear it in my accent when I talk, I'm an Englishman in New York. You see me walking down 5th Avenue, walking cane here at my side. I take it everywhere I walk, I'm an Englishman in New York. Oh, I'm an alien, I'm a legal alien, I'm an Englishman in New York. If manners make a man, as someone said, he's our hero of the day. It takes a man to suffer ignorance and smile, be yourself no matter what they say. Oh, I'm an alien, I'm a legal alien, I'm an Englishman in New York. Mothers keep on writing, the leaves are going to rise, but you put them up as the only one. No love so bright, a man in this society, the man in candles brighter than the sun. It takes more than combat gear to make a man, it takes more than a license for a gun. Confront your enemies, avoid them when you can, a gentleman will walk but never run. If manners make a man, as someone said, he's our hero of the day. It takes a man to suffer ignorance and smile, be yourself no matter what they say. Oh, I'm an alien, I'm a legal alien, I'm an Englishman in New York. Oh, I'm an alien, I'm a legal alien, I'm an Englishman in New York.\"\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}